---
title: "An Introduction to INLA-SPDE in R"
author: "Misha Tseitliani"
date: "2025-11-25"
output: html_document
bibliography: refINLA.bib
csl: nature.csl
---

# Background

At this point, we have gone through a number of methods for processing spatial data. Many of these were **non-parametric** or **semi-parametric**: this means that it can be difficult to link specific changes in a parameter or covariate to the intended result. In addition, many have been **spatially implicit**: they use coordinates as a proxy for spatial structure rather than modelling some *true space*. INLA can do all these too but is especially powerful for **parametric** and **spatially explicit** models. If you want to conduct inference on your data, then this might be perfect for you!

## Definitions

Before going too deep into the package details, first we need to ensure we're familiar with the language of spatial statistics. The point is not to teach theory (which will mostly be omitted) but to ensure we have a *shared language* when questions arise. If you *are* interested in the theory, there are numerous good [resources](https://www.paulamoraga.com/book-spatial/) out there.[@blangiardo_spatial_2015; @moraga_geospatial_2019; @moraga_spatial_2023]

INLA is theoretically inspired by hierarchical and state space models and has both **latent** and **observed** processes:

* **latent process**: the *true* environmental reality (e.g., number of species in a sampling plot). Also called a **state process**.
* **observation process**: what we see with our data (e.g., the number of species *recorded* in a sampling plot). With a normal model, this is what you are looking at.

With that sorted, spatial modelling has vocabulary corresponding to the geospatial language you know (and maybe love):

* **(Poisson) point process**: a 2D distribution for **point vector data** and the basis of many geospatial models.
* **Cox process**: a doubly stochastic Poisson process for **point vector data** where the intensity (i.e., expected number of points in a specific area) is itself stochastically random. INLA has special functions for Cox and LGCP (log-Gaussian Cox process) models.
* **Gaussian random field**: a continuous 2D field for **raster data** or **fully defined vector polygons**. In essence, fields are the **latent process** from which you draw point process distributions (via sampling).
* **Gauss-Markov random field (GMRF)**: a Gaussian random field for **raster data**  or **fully defined vector polygons** where values structurally depend on some specific distance, time, etc (i.e., Markovian dependencies) but are otherwise independent.

To reiterate, INLA's spatial models are directly based on GMRFs, and point process models are treated as drawn from some latent GMRF. This means that you can always include raster data (e.g., as covariates) directly in your models even if your response is vector data.

### So, INLA (and SPDE)?

**Integrated Nested Laplace Approximations (INLA)** are a method of estimating Bayesian models that run faster than MCMC (Markov Chain Monte Carlo) while still generating similar results. Spatial models in the `INLA` package use the **SPDE (Stochastic Partial Differential Equations)** approach to better fit spatial structures: hence the name INLA-SPDE. These are fit over a spatial mesh using splines to run faster. In practice, users need only specify a spatial mesh and regression formulation (i.e., INLA's models are linear additive regression models) to get results.

If you plan to be a casual user at best, you can consider the basic SPDE field as a 2D random effect controlling for spatial patterns and move on.

## Assumptions

As a general rule of thumb, INLA-SPDE models are more rigid, spatially explicit, and accessible than MCMC. They also run much faster. In exchange, they come with a few assumptions:

* **Stationarity**: the underlying stochastic process is space-invariant (and time-invariant in spatiotemporal models). This would mean that your latent field should have a consistent mean and variance across the sampling domain. In practice, `INLA` has a default parameter `alpha = 2` that lets SPDEs accommodate some non-stationarity.[@ingebrigtsen_spatial_2014; @gomez-rubio_bayesian_2020]
* **Isotropy**: covariance depends only on the distance between points (i.e., the spatial field is rotation-invariant). This assumption is quite strict, but some exceptions (e.g., diffusion and barrier models) are possible using supplemental packages like `INLAspacetime`. If you want to model impassable barriers (e.g., fences), see [this vignette](https://eliaskrainski.github.io/INLAspacetime/articles/web/barrierExample.html) for more information. There is debate on how much this impacts edge/boundary effects, but best practice is to define your study area to be a bit larger just in case. [@lindgren_bayesian_2015]

These two major assumptions are largely theoretical and are often violated by real-world data and scientific publications. In the worst case, just note them and move on. In the best case, find a way to sub-sample your data, choose a more suitable `INLA` formulation, or find a more flexible modelling package.

Beyond those, simple models function similarly to GAMMs (generalised additive mixed models) and include those assumptions:

* **Conditional independence**: observations are independent aside from the specified spatial and covariate structures.
* **Appropriate mean-variance relationships**: you are using the correct distribution and specifying its parameters appropriately. Don't model binary data with a Gaussian, for example.
* **Link-scale linearity**: with the exception of more complicated hierarchical models, all your covariates will be in a single equation added together. Is this reasonable?
* **Separability (e.g., collinearity)**: *for inference only*, you should avoid adding multiple terms that correlate with each other. In practice, you'll end up with untrustworthy estimates for any term that has this problem.

### Bayesian Statistics: An Aside

Unlike many traditional regression models, `INLA` is Bayesian, which means that you can set priors at almost any step in the modelling. See the first chapter of [this handy online book](https://becarioprecario.bitbucket.io/inla-gitbook/ch-intro.html#bayesian-inference) if you want more equations (like Bayes Theorem) and theoretical explanations. As a cursory review, Bayesian models are made up of **priors**, **likelihoods**, and **posteriors**:

* **Priors** are initial beliefs about the stochastic processes (and can include specialised geographical knowledge gained from the field).
* **Likelihoods** are the output of the data-based evaluation. Frequentist regression results (i.e., `glm`, `nlme`, `lme4`) *are* the likelihood.
* **Posteriors** combine the prior and likelihood to give what we think is true reality.

A good rule of thumb is to **always prefer default priors unless you have a reason not to**. For advanced model tweaks though, it can get quite interesting. We will go through some prior checks at the end of our worked example to demonstrate how you can interpret if, for example, you're using the correct distribution family (e.g., Gaussian, Poisson). INLA priors often work through **hyperparameters**: e.g., $\mu$ and $\sigma$ in Gaussian distribution.

The most likely priors you might use for bigger datasets are **penalised complexity priors** which tell the posterior to prefer simpler models whenever possible. In practice, these help models fit faster (even though `INLA` is already fast), and you can interpret the posterior more easily.

# Worked Example

That's enough theory; let's apply this to real data. This section will walk through an INLA-SPDE modelling pipeline from package installation to model diagnostics and comparison.

### Package Installation

```{r global, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First, we'll need `tidyverse`, `INLA`, `fmesher`, and `inlabru` packages for this example. `INLA`, however, is not on CRAN and must be downloaded manually. I provide one example, but other options are available at the [`inlabru` homepage](https://inlabru-org.github.io/inlabru/index.html) and the [`INLA` website](https://www.r-inla.org/download-install). These are **highly dependent on your operating system and ICT permissions on your computer**, but ICT (or I) can help find an option that works. At my last check, the example below should work on a newer Windows laptop with no admin permissions.

```{r imports, eval = FALSE}
# install INLA following the online instructions
# Finn reccomends installing the testing version due to lots of updates recently
# but you can always replace with the stable version if you'd like
install.packages("INLA",repos=c(getOption("repos"),INLA="https://inla.r-inla-download.org/R/testing"), dep=TRUE) 

# install inlabru
install.packages("inlabru")
# install fmesher
install.packages("fmesher")
```

Now, we can get our working environment in order.

```{r setup, results = FALSE, message = FALSE, warning = FALSE}
# set working directory to the root folder of the project I'm working in 
setwd(here::here())

# activate the packages as needed
library(tidyverse)
library(INLA)
library(inlabru)
library(fmesher)
# anything spatial relies on sf
library(sf)
```

### Data Checks

We'll use the Kilpisjärvi data from previous examples to model species richness. `INLA` can handle NAs in covariate data but not in the response, so make sure to check for messier data. There is nothing of concern for us if we're modelling `species_rich` as our response.

```{r naTest, , message = FALSE}
# import the data
kjRaw <- read_csv("Kilpisjarvi.csv")

# NAs per column for the three columns of interest
t(kjRaw %>% select(c("Lat", "Lon", "species_rich")) %>% 
    summarise(across(everything(), ~ sum(is.na(.)))))
```
### Spatial Setup

You can skip this if you're fitting a non-spatial model (e.g., GLM or GLMM), but if you're stopping there then why use `INLA` in the first place?

First thing we need to do is set up the spatial structure in the data itself because working with `sf` objects makes everything easier.

```{r data, results = FALSE}
# make spatial
kjGeo <- st_as_sf(kjRaw, 
                  coords = c("Lon", "Lat"),
                  crs = 4326)
```

SPDE models start with the mesh, implemented using `fmesher`. At its simplest, we can do this automatically with `sf` objects or raw coordinates. In general, the more mesh nodes there are, the longer the model will take to run. If you want to test lots of models without worrying too much about *how well the estimation works*, coarse meshes are best. 

`fmesher` started as a function within INLA, so older code uses slightly different syntax (more information on converting via the [vignette](https://inlabru-org.github.io/fmesher/articles/inla_conversion.html)). These days `fmesher` can be used to specify spatial fields for other packages like `tinyVAST` and `sdmTMB` too; other packages like `gllvm` and `Hmsc` are working on compatability. `fmesher` builds a mesh based on triangles, where values (e.g., of covariates, posterior estimates, spatial fields) are treated as the same at all points within a single triangle. It's not *truly* continuous in a strict sense, but you can get very close by creating finer and finer meshes.

I'll show a basic, default mesh using the base method. Some handy alternatives (all with the `loc =` argument):

* `loc = ` can generate spherical meshes for global models if given 3D coordinates. 2D coordinates are the norm for `sf` objects.
* `fm_nonconvex_hull()` works with non-convex spatial areas that defaults handle poorly.
* `fm_hexagon_lattice()` is Finn's recommendation for a baseline mesh but has been (in his words) ignored by most modellers because it wasn't the default nor in the vignettes until this year. It represents a spatially uniform sampling protocol.

```{r mesh, message = FALSE, fig.width = 10, fig.height = 8, fig.align = "center"}
meshBasic <- fm_mesh_2d_inla(
  # you can also set this with a boundary shapefile (in sf format) using
  #boundary = kjBoundary
  # location or boundary, you can only pick one!
  loc = kjGeo$geometry, 
  # set map units inside and outside 
  max.edge = c(0.9, # smaller inner units give a more detailed mesh within the sampling area
               1), # smaller outer units give more detailed spatial estimates of boundary effects
  # you can also set this to a single number like if you aren't worried about edge effects, for example:
  #max.edge = 1
  # cutoff is equivalent to the min edge
  #cutoff = 2, 
  crs = fm_crs(kjGeo)
)

# plotting is easy; you should always visually confirm
meshPlot <- ggplot() + 
  geom_fm(data = meshBasic) + 
  geom_sf(data = kjGeo, size = 0.5, colour = "red") + 
  theme_bw()
meshPlot
```

Next, we make the SPDE object for spatial models. We again use the base function, but `inla.spde2.pcmatern` is a nice alternative when you're working with big data: you can set **penalised complexity priors** to avoid bloat in the spatial structure. Our data is quite compact already, so this isn't necessary here.

You can add additional priors on spatial scale/range ($\kappa$ or $\rho$) and variance ($\tau$ or $\sigma$), depending on whether you're using the [`matern`](https://rdrr.io/github/INBO-BMK/INLA/man/inla.spde2.matern.html) or [`pcmatern`](https://rdrr.io/github/INBO-BMK/INLA/man/inla.spde2.pcmatern.html) functions, respectively. There is also an $\alpha$ scalar directly relating to the SPDE's smoothness (since it's fit using splines) between 0 and 2. $\alpha$ is 2 by default in the 2D case, and you'll rarely need to mess with it for normal spatial models.

As you might tell from the function name, these 2D meshes (and lots of other spatial models in `INLA`) are based on the Matérn covariance function.

```{r spdeSetup, results = FALSE}
# quantifies distance between points and define Matern correlation on the mesh for SPDE
matern2D <- inla.spde2.matern(mesh = meshBasic)
```

And that's it! You can import data and set up a simple model with just 10 lines of code.

### Model Formulation

We're working with species richness, which is positive discrete (i.e., no values below zero and no decimal values). It varies between `r min(kjRaw$species_rich)` and `r max(kjRaw$species_rich)`.

```{r richPlot, message = FALSE, fig.align = "center", echo = FALSE}
richPlot <- ggplot(kjRaw, aes(x = species_rich)) + geom_histogram() + theme_bw() + 
  xlab("Species richness") + ylab("Count")
richPlot
```

INLA supports an dizzying amount of distribution families that aren't well [documented](https://inla.r-inla-download.org/r-inla.org/doc/likelihood/). For our case, [Gaussians](https://inla.r-inla-download.org/r-inla.org/doc/likelihood/gaussian.pdf) are a decent baseline choice but cannot be constrained to positive values only. [Poisson](https://inla.r-inla-download.org/r-inla.org/doc/likelihood/poisson.pdf) is a good alternative (especially [non-zero Poisson](https://inla.r-inla-download.org/r-inla.org/doc/likelihood/nzpoisson.pdf)) though dispersion is always a concern. [Negative binomial distributions](https://inla.r-inla-download.org/r-inla.org/doc/likelihood/nbinomial.pdf) can present another good option. 

In this case, I use the [Tweedie distribution](https://inla.r-inla-download.org/r-inla.org/doc/likelihood/tweedie.pdf)--which includes Poisson-Gamma, Gaussian, gamma, and quasi-Poisson. Tweedie particularly suits positive continuous and right-skewed distributions that may include zero inflation.[@gilchrist_use_2000] It comes with two hyperparameters: $\theta_1$, power, and $\theta_2$, log(dispersion). Power $p$ is actually $1 + \frac{\exp(\theta_1)}{1+\exp(\theta_1)}$. So when $p = 2$, we get a [quasi-Poisson](https://bookdown.org/mike/data_analysis/sec-quasi-poisson-regression.html) (Poisson with some dispersion); $p = 1$ gives us a [gamma distribution](https://library.virginia.edu/data/articles/getting-started-with-gamma-regression).

In short, the Tweedie is flexible and relatively easy to interpret. It takes a log-link in INLA, so make sure to exponentiate your coefficients on the response scale.

Remember earlier that I mentioned the dangers of collinearity in linear additive regression models used for inference. To decide which covariates to include in our model, we look at the correlation plot. In reality, collinearity is better checked after model fit, but correlation can sometimes (not always) give a good early warning of possible issues. We decide that the community-weighted means (CWM) of leaf dry matter content (LDMC) and leaf area are too closely correlated. Same issue for mean temperature and freezing degree days (FDD). 

```{r covCor, echo = FALSE, message = FALSE, fig.align = "center"}
ggplot(reshape2::melt(cor(kjRaw %>% select_if(is.numeric) %>% select(-c("Lat","Lon","N_TM35FIN","E_TM35FIN",
                                                              "Gersyl_max_height", "Gersyl_LDMC", "Gersyl_leaf_area",
                                                              "Saxifraga_cernua","Rhododendron_lapponicum","Arctous_alpina",
                                                              "Dryas_octopetala","Cornus_suecica","Vaccinium_myrtillus",
                                                              "Empetrum_nigrum","Bistorta_vivipara","Geranium_sylvaticum",
                                                              "Trollius_europaeus")),
                use = "complete.obs")), 
       aes(Var1, Var2, fill=value)) +
  geom_tile(height=0.8, width=0.8) +
  scale_fill_gradient2(low="#fde725", mid="#21918c", high="#440154", midpoint = 0.55) +
  theme_minimal() +
  coord_equal() +
  labs(x="",y="",fill="Correlation") +
  theme(axis.text.x=element_text(size=13, angle=45, vjust=1, hjust=1, 
                                 margin=margin(-3,0,0,0)),
        axis.text.y=element_text(size=13, margin=margin(0,-3,0,0)),
        panel.grid.major=element_blank()) 
```

Finally, modern modelling packages are increasingly sensitive to scaled versus unscaled covariates, and `INLA` is no exception. What to rescale and how is complicated for complex structures (e.g., `sf` and `terra` spatial covariates; temporal structures), but it's straightforward when we have our data as a normal dataframe like this. This does, however, affect model interpretation, so we want to retain the original means and variances for interpretation later.

```{r norm, message = FALSE}
# rescale manually because scale() is a pain to deal with using tidyverse
kjGeoNorm <- kjGeo %>% mutate(across(c("cwm_max_height", "cwm_LDMC", "FDD_T3", "GDD3_T3", "median_moist"), 
                                     ~ (. - mean(., na.rm = TRUE)) / sd(., na.rm = TRUE)))


# save the mean and standard deviation to convert back for interpretation
mean <- as.data.frame(t(kjRaw %>% select(c("cwm_max_height", "cwm_LDMC", "FDD_T3", "GDD3_T3", "median_moist")) %>% 
    summarise(across(everything(), ~ mean(., na.rm = TRUE))))) %>% rename(mean = "V1")
sd <- as.data.frame(t(kjRaw %>% select(c("cwm_max_height", "cwm_LDMC", "FDD_T3", "GDD3_T3", "median_moist")) %>% 
    summarise(across(everything(), ~ sd(., na.rm = TRUE))))) %>% rename(sd = "V1")

metaStats <- cbind(mean, sd)
metaStats
```

### Model Running

`inlabru` syntax is relatively easy, and we'll ignore the complex options for now. I personally like the `bru_obs()` intermediate function, but it's optional in the newer versions. The `options = list()` argument for `bru()` has too many options to list out, but some that I've used before (and liked) include:

* `lincomb`: maybe you're interested in a linear combination of covariates (e.g., grazing by two different species) which you can add
* `quantiles`: are you unsatisfied with the median or want easier access to credible levels (e.g., 95% CI)?
* `control.compute`: what metrics do you want for model evaluation? CPO, WAIC, DIC, and marginals predictors can all be selected
* `control.predictor`: what do you want for model prediction, if anything?
* `control.inla`: manually choose the fitting strategy (e.g., empirical Bayes)
* `bru_verbose`: set to TRUE for basic messages about the fit; set to 2 or 3 for even more details

First, I show a basic non-spatial GLM with `inlabru` syntax. Compare with `bayesreg` or any other regression method of your choice if you'd like.


```{r baseModel, message = FALSE}
# write the formula
formulaGLM <- species_rich ~ Intercept(1) + cwm_max_height + cwm_LDMC + 
  FDD_T3 + GDD3_T3 + median_moist

# run the model
fitGLM <- bru(formulaGLM, kjGeoNorm, family = "tweedie"
              # I really like custom quantiles, so I show the syntax here
              #options = list(quantiles = c(0.025, 0.05, 0.5, 0.95, 0.975)))
              )
```

You can also fit GLMMs with `inlabru` by adding `field(site, model = "iid")` to the formula. Here, I don't bother running it given the lack of grouped survey design variables.

Spatial models aren't that much more complex to build and run. The main difference is the `field` argument, which is the engine of all complex modelling in `inlabru`. Matérn spatial fields with time lags and mixed effects (including hierarchical designs) are among the "simpler" possible designs. There's also no limit on the number of effects, so you can have survey random effects and a spatial random field in the same model. Just make sure to name each effect differently (e.g., `field1()`, `field2()`, `field3()`). As a warning, models with too many fields (or improperly specified ones) may not converge and spit out a warning while failing. The more niche your structure, the more thinking and debugging is needed to ensure that things work OK.

```{r spatModel}
# write the formula
formulaGeo <- species_rich ~ Intercept(1) + cwm_max_height + cwm_LDMC + 
  FDD_T3 + GDD3_T3 + median_moist + 
  # new stuff
  field(geometry, model = matern2D)
  # to demonstrate the syntax for adding a second effect
  # + field2(site, model = "iid")

# run the model
fitGeo <- bru(formulaGeo, kjGeoNorm, family = "tweedie")
```

`inlabru` *can* take more complex covariate structures, but the model specification bloats quickly and you'll need to use the `bru_obs()` (previously called `like()`) between specifying the formula and fitting the model in order to set a more detailed likelihood. A lot of this functionality is new (especially for `terra` objects), but there are a examples of how to include [spatial covariates in point process models](https://inlabru-org.github.io/inlabruCourseMay2025/articles/lgcp_2d_covars.html). Syntax is broadly the same. I give an example below using the same syntax for our spatial field models. Posterior checks are left as an exercise to the reader because of syntax bloat.

It's important to note that any *special* covariates you choose to include, `field` notation is essential. And below is an excellent demonstration that you can call then whatever you want. However, when using this syntax, the `model = ` argument tells `inlabru` how to evaluate the object (especially if using a different data source outside the main dataframe). A very poorly organised list of `model` options can be found in the depths of [original `INLA` documentation](https://inla.r-inla-download.org/r-inla.org/doc/latent/). If just trying to fit a fixed effect, `model = "linear"` should work for `terra` objects. If having trouble with spatial objects, play around with manual options in `eval_spatial()` which usually runs in the background.

```{r rastModel, eval = FALSE}
# convert the mesh back to sf
sfMesh <- st_as_sf(as.data.frame(meshBasic$loc), 
                 coords = c("V1", "V2"),
                 crs = 4326)
# get bounding box
meshBounds <- st_as_sfc(st_bbox(test), crs = 4326)
# extract centroids for elevatr evaluation
meshCentre <- st_centroid(sfMesh)

library(elevatr)
library(terra)
# arbitrary call for demonstrative purposes
elevRL <- get_elev_raster(locations = meshCentre, z = 5)
# convert from RasterLayer to SpatRaster
# elevatr's GitHub says they're changing output object type, so maybe unnecessary in the future
elevT <- crop(terra::rast(elevRL, "epsg:4326"), 
              # crop to bounding box for evaluation
              meshBounds)
# de-mean elevation (copying the example article, but changing the variance may be good too)
elev <- elevT - mean(terra::values(elevT), na.rm = TRUE)

# define the model
formRast <- species_rich ~ elev(elev, # syntax is more complex if your data object has more than 1 layer
                                model = "linear") +
  # add previous model stuff here
  cwm_max_height + cwm_LDMC + FDD_T3 + GDD3_T3 + median_moist + 
  field(geometry, model = matern2D) + Intercept(1)
# run the model
fitGeoRast <- bru(formRast, kjGeoNorm, family = "tweedie")
```

## Understanding Output

Bayesian models, and especially advanced packages like `inlabru` lack an agreed-upon paradigm for interpretation and evaluation. It's your job as a modeller and scientist to understand what the numbers are saying, why it may matter, and what the pitfalls are. INLA isn't really that different from any other spatial Bayesian model, so you can absolutely steal my code while using your own geographer's knowledge to draw conclusions.

Broadly, `INLA` models have two ways to look at output:

* the information contained directly in the model object like residuals (and other stuff added using `control.compute` and `control.predictor`)
* a newer `predict` function that works only with `inlabru` and not legacy `INLA` models

Both are useful and your focus will depend on the research question (i.e., prediction or inference). The model object itself is quite big, and I won't even pretend to give a deep overview of its contents. These can be used for both interpretation and diagnostics depending on what's of interest.

```{r names}
names(fitGeo)
```

### Interpretation

The following section contains in-line code evaluation. If you're confused about where a number came from, consult the `.rmd` file in [the GitHub repo](https://github.com/MishaTs/DP00BB11_INLA). This section starts on line 327.

Start with the summary function. It gives a good overview.

```{r sum}
summary(fitGeo)
```

In it is most of the non-spatial information we need. Contrary to what you might think, the only *truly spatial* part of our `inlabru` model is the random `field` itself. Everything else functions as a normal scalar via normal regression rules. We demonstrate how to interpret the Intercept and one coefficient:

* **Intercept**: the typical site species richness when all covariates are at their mean values is `r round(exp(fitGeo$summary.fixed$mean[1]), 2)` with a [`r round(exp(fitGeo$summary.fixed[1,3]), 2)`, `r round(exp(fitGeo$summary.fixed[1,5]), 2)`] 95% credible interval
* **FDD_T3**: when all other covariates are at their mean values, an extra freezing degree day leads to `r signif(exp(fitGeo$summary.fixed$mean[2])/sd(kjRaw$FDD_T3, na.rm = TRUE), 2)` `r ifelse(fitGeo$summary.fixed$mean[2] >= 0, "higher", "lower")` species richness with a [`r signif(exp(fitGeo$summary.fixed[2,3])/sd(kjRaw$FDD_T3, na.rm = TRUE), 2)`, `r signif(exp(fitGeo$summary.fixed[2,5])/sd(kjRaw$FDD_T3, na.rm = TRUE), 2)`] 95% credible interval

The specific formula to convert a coefficient output to "real world" language is `exp(inlaCoefficient)/sd(originalColumn)`. For FDD_T3, this is `exp(fitGeo$summary.fixed$mean[2])/sd(kjRaw$FDD_T3, na.rm = TRUE)`. Rinse and repeat with the credible interval bounds that you're interested in since they convert the same as coefficients. If you want to use the original values, you can say that a 1 standard deviation increase in FDDs leads to an `exp(inlaCoefficient)`, in this case `r signif(exp(fitGeo$summary.fixed$mean[2]), 2)`, `r ifelse(fitGeo$summary.fixed$mean[2] >= 0, "higher", "lower")` species richness. If you prefer social science language, then a 1 standard deviation increase in FDDs leads to a `r abs(signif(fitGeo$summary.fixed$mean[2], 2))`% `r ifelse(fitGeo$summary.fixed$mean[2] >= 0, "increase", "decrease")` in species richness (just the actual model coefficient itself). Communicate in the way that feels best to you.

Remember, we are exponentiating because the Tweedie distribution takes a log link function in `INLA`. You can skip this step if using a distribution with a linear link, like Gaussian. If you're working with logistic link functions, I can only wish you good luck. Things also change a bit if you rescale or log your response variable (which can be done), but the general interpretative rules stay the same: consult these handy StackExchange posts for [quantiative](https://stats.stackexchange.com/questions/603213/interpretation-of-rescaled-coefficients-in-ols) and [language](https://stats.stackexchange.com/questions/407822/interpretation-of-standardized-z-score-rescaled-linear-model-coefficients) tips on interpreting models where both response variables and covariates are rescaled.

There is no "significance" in Bayesian statistics in the same way as classical methods, but it's quite common to say that a 95% credible interval excluding zero shows meaningful evidence of an effect. This is why I suggested (and provided code for) different quantiles in the `bru` call: you can also do this at the 90% credible level, for example.






Dealing with the spatial field is a bit more complicated.




Bayes factor is another "robust" measure for inference which is the ratio of the marginal likelihoods (`mlik`) of two models. From the model object, you want the ["log marginal-likelihood (integration)"](https://groups.google.com/g/r-inla-discussion-group/c/1Zs00DttjEE). So if you want to measure the relative importance of a single term, you must fit models with and without the term of interest. [Wikipedia explains](https://en.wikipedia.org/wiki/Bayes_factor#Interpretation) how to interpret this well with good scientific references.





Hyperparameters, too, explain the model suitability and distributional behaviour.






### Diagnostics

"Since all models are wrong the scientist must be alert to what is importantly wrong." - George Box[@box_science_1976]

Stationarity (weak) and isotropy (strong) assumptions are best tackled theoretically by most users. Are we missing any linear fixed effects or spatial variance terms that would affect the spatial patterns across the pixels/points (i.e., stationarity) or across the whole study area in larger space (i.e. isotropy)? For example, maybe georichness is a strong spatial explainer of species richness in the Kilpisjärvi area that we do not account for via basic topography. Ordination methods (e.g., `gllvm`) can offer a nice way to examine the unmodelled structures.

Predictive scoring, if that's your focus, is a complex topic, and `inlabru` has a nice theoretical description and links to another package, `scoringRules` that can provide more help if you consult [the official article](https://inlabru-org.github.io/inlabru/articles/prediction_scores.html). Another older resource is [the INLA book](https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#sec:modelassess). These generally require some testing set (data that we do not use to fit the model but generated in the same way) or at least a theoretically generated comparison dataset. Both of these are too much work for this exercise. In my experience, CPRS is often treated as a "gold standard" for predictive metrics. Sometimes, these are used as model evaluation for inference as well due to their good theoretical and practical implementation.

Easier options to look at are CPO, WAIC, and DIC although information criteria can have issues depending on model components (e.g., [point process observation model likelihoods](https://groups.google.com/g/r-inla-discussion-group/c/EppMtvdJ610/m/YwjpRvoAAgAJ)). These are not computed by default and must be selected using `control.compute`, which we did not do.



## Wrap-Up

`INLA` is a powerful method where you can fit a dizzying variety of models. I don't cover it here, but it can even accommodate spatio-temporal data, either through mixed effects or autoregressive structures in the spatial field. More complex spatial relationships are also possible like [spatially varying covariates (SVC)](https://inlabru-org.github.io/inlabru/articles/svc.html). For more information on the options, explore [the package website](https://inlabru-org.github.io/inlabru/index.html). 

Now is a time of change for `inlabru` as the package is going through major updates and developments to make it easier to use and understand. Last time I ran these models a year ago, the syntax was almost 5 times as long. Before `inlabru` came out, the original `INLA` code took over 100 lines for a single model, including defining matrices manually. If you plan to consult these notes 3-4 years from now, it's worth checking the website first.

Feel free to [email me](mailto:Michael.Tseitlin@oulu.fi) or ask today/tomorrow if you think some of these apply to your own research and you want more help.

If you'd rather get help from the source, `INLA` has [a great Q&A discussion forum](https://groups.google.com/g/r-inla-discussion-group) where Finn and Håvard (the original authors and package developers) regularly comment and troubleshoot code from the community. When in doubt, just ask! It's also possible you're not the first one to have a problem, so maybe you'll just find an answer by searching through it.

## Acknowledgements

Many of the links, references, and materials came from [Ben Swallow](https://ben-swallow-research.github.io/) during his supervison of my MSc dissertation. The theory and background relied on older slides from [Janine Illian](https://www.gla.ac.uk/schools/mathematicsstatistics/staff/janineillian/). Newer `inlabru` materials came from Finn Lindgren's [2025 `inlabru` course](https://inlabru-org.github.io/inlabruCourseMay2025/index.html).

# References
