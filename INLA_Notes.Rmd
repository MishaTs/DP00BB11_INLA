---
title: "An Introduction to INLA-SPDE in R"
author: "Misha Tseitliani"
date: "2025-11-25"
output: html_document
bibliography: refINLA.bib
csl: nature.csl
---

# Background

At this point, you have gone through a number of methods for processing spatial data. Many of these were **non-parametric** or **semi-parametric**: this means that it can be difficult to link specific changes in a parameter or covariate to the intended result. In addition, many have been **spatially implicit**: they use coordinates as a proxy for spatial structure rather than modelling some *true space*. INLA can do all these too but is especially powerful for **parametric** and **spatially explicit** models. If you want to conduct inference on your data, then this might be perfect for you!

## Definitions

Before going too deep into the package details, first I introduce the language of spatial statistics. The point is not to teach theory (which will mostly be omitted) but to ensure we have a *shared language* when questions arise. If you *are* interested in the theory, there are numerous good [resources](https://www.paulamoraga.com/book-spatial/) out there.[@blangiardo_spatial_2015; @moraga_geospatial_2019; @moraga_spatial_2023]

INLA is theoretically inspired by hierarchical and state space models and has both **latent** and **observed** processes:

* **latent process**: the *true* environmental reality (e.g., number of species in a sampling plot). Also called a **state process**.
* **observation process**: what's measured in the data (e.g., the number of species *recorded* in a sampling plot). With a normal model, this is what you are looking at.

With that sorted, spatial modelling has vocabulary corresponding to the geospatial language you know (and maybe love):

* **(Poisson) point process**: a 2D distribution for **point vector data** and the basis of many geospatial models.
* **Cox process**: a doubly stochastic Poisson process for **point vector data** where the intensity (i.e., expected number of points in a specific area) is itself stochastically random. INLA has special functions for Cox and LGCP (log-Gaussian Cox process) models.
* **Gaussian random field**: a continuous 2D field for **raster data** or **fully defined vector polygons**. In essence, fields are the **latent process** from which you draw point process distributions (via sampling).
* **Gauss-Markov random field (GMRF)**: a Gaussian random field for **raster data**  or **fully defined vector polygons** where values structurally depend on some specific distance, time, etc (i.e., Markovian dependencies) but are otherwise independent.

To reiterate, INLA's spatial models are directly based on GMRFs, and point process models are treated as drawn from some latent GMRF. This means that you can always include raster data (e.g., as covariates) directly in your models even if your response is vector data.

### So, INLA (and SPDE)?

**Integrated Nested Laplace Approximations (INLA)** are a method of estimating Bayesian models that run faster than MCMC (Markov Chain Monte Carlo) while still generating similar results. Spatial models in the `INLA` package use the **SPDE (Stochastic Partial Differential Equations)** approach to better fit spatial structures: hence the name INLA-SPDE. These are fit over a spatial mesh using splines to run faster. In practice, users need only specify a spatial mesh and regression formulation (i.e., INLA's models are linear additive regression models) to get results. 

If you plan to be a casual user at best, you can consider the basic SPDE field as a 2D random effect controlling for spatial patterns and move on.

And as a side note on speed and computation, `INLA` has supported parallelisation by default since 2022 and methods have had a `num.threads` [since 2016](https://groups.google.com/g/r-inla-discussion-group/c/0dlvlVyiIRQ).[@gaedke-merzhauser_parallelized_2022] Defaults are usually good, and manual thread control is more often used to limit rather than increase resource usage. HPC use is supported; installation can go through [Apptainer](https://inlabru-org.github.io/inlabru/articles/Apptainer.html) and memory sharing through [OpenMP](https://groups.google.com/g/r-inla-discussion-group/c/jshuUG_T-TE/m/GiUUo72uAAAJ). Both of these are supported on CSC systems. The University of Oulu recently launched a new in-house HPC environment Lehmus, but the [documentation](https://ict.oulu.fi/22819/?page&lang=en) doesn't mention either of these (or Docker). You'll need to contact [ICT's general help](mailto:ict@oulu.fi) or dedicated Lehmus staff [Jesse Korhonen](mailto:Jesse.Korhonen@oulu.fi) for answers.

## Assumptions

As a general rule of thumb, INLA-SPDE models are more rigid, spatially explicit, and accessible than MCMC. They also run much faster. In exchange, they come with a few assumptions:

* **Stationarity**: the underlying stochastic process is space-invariant (and time-invariant in spatiotemporal models). This would mean that your latent field should have a consistent mean and variance across the sampling domain. In practice, `INLA` has a default parameter `alpha = 2` that lets SPDEs accommodate some non-stationarity.[@ingebrigtsen_spatial_2014; @gomez-rubio_bayesian_2020]
* **Isotropy**: covariance depends only on the distance between points (i.e., the spatial field is rotation-invariant). This assumption is quite strict, but some exceptions (e.g., diffusion and barrier models) are possible using supplemental packages like `INLAspacetime`. If you want to model impassable barriers (e.g., fences), see [this vignette](https://eliaskrainski.github.io/INLAspacetime/articles/web/barrierExample.html) for more information. There is debate on how much this impacts edge/boundary effects, but best practice is to define your study area to be a bit larger just in case. [@lindgren_bayesian_2015] To be clear, anisotropy is not *impossible* but remains [unimplemented using `INLA` specifically](https://groups.google.com/g/r-inla-discussion-group/c/miTLL2VmA6U/m/GklhxbwLAgAJ).

These two major assumptions are largely theoretical and are often violated by real-world data and scientific publications. In the worst case, just note them and move on. In the best case, find a way to sub-sample your data, choose a more suitable `INLA` formulation, or find a more flexible modelling package.

Beyond those, simple models function similarly to GAMMs (generalised additive mixed models) and include those assumptions:

* **Conditional independence**: observations are independent aside from the specified spatial and covariate structures.
* **Appropriate mean-variance relationships**: you are using the correct distribution and specifying its parameters appropriately. Don't model binary data with a Gaussian, for example.
* **Link-scale linearity**: with the exception of more complicated hierarchical models, all your covariates will be in a single equation added together. Is this reasonable?
* **Separability (e.g., collinearity)**: *for inference only*, you should avoid adding multiple terms that correlate with each other. In practice, you'll end up with untrustworthy estimates for any term that has this problem.

### Bayesian Statistics: An Aside

Unlike many traditional regression models, `INLA` is Bayesian, which means that you can set priors at almost any step in the modelling. See the first chapter of [this handy online book](https://becarioprecario.bitbucket.io/inla-gitbook/ch-intro.html#bayesian-inference) if you want more equations (like Bayes Theorem) and theoretical explanations. As a cursory review, Bayesian models are made up of **priors**, **likelihoods**, and **posteriors**:

* **Priors** are initial beliefs about the stochastic processes (and can include specialised geographical knowledge gained from the field).
* **Likelihoods** are the output of the data-based evaluation. Frequentist regression results (i.e., `glm`, `nlme`, `lme4`) *are* the likelihood.
* **Posteriors** combine the prior and likelihood to give what would be true reality.

A good rule of thumb is to **always prefer default priors unless you have a reason not to**. For advanced model tweaks though, it can get quite interesting. INLA priors often work through **hyperparameters**: e.g., $\mu$ and $\sigma$ in Gaussian distribution. I will show hyperparameter checks at the end of the example to demonstrate how you can interpret if you're using the correct distribution family (e.g., Gaussian, Poisson) based on posterior hyperparameters.

The most likely priors you might use for bigger datasets are **penalised complexity priors** which tell the posterior to prefer simpler models whenever possible. In practice, these help models fit faster (even though `INLA` is already fast), and you can interpret the posterior more easily.

# Worked Example

That's enough theory; let's apply this to real data. This section will walk through an INLA-SPDE modelling pipeline from package installation to model diagnostics and comparison.

### Package Installation

```{r global, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First, you'll need `tidyverse`, `INLA`, `fmesher`, and `inlabru` packages for this example. `INLA`, however, is not on CRAN and must be downloaded manually. I provide one example, but other options are available at the [`inlabru` homepage](https://inlabru-org.github.io/inlabru/index.html) and the [`INLA` website](https://www.r-inla.org/download-install). These are **highly dependent on your operating system and ICT permissions on your computer**, but ICT (or I) can help find an option that works. At my last check, the example below should work on a newer Windows laptop with no admin permissions.

```{r imports, eval = FALSE}
# install INLA following the online instructions
# Finn reccomends installing the testing version due to lots of updates recently
# but you can always replace with the stable version if you'd like
install.packages("INLA",repos=c(getOption("repos"),INLA="https://inla.r-inla-download.org/R/testing"), dep=TRUE) 

# install inlabru
install.packages("inlabru")
# install fmesher
install.packages("fmesher")
```

Next is setting up the working environment.

```{r setup, results = FALSE, message = FALSE, warning = FALSE}
# set working directory to the root folder of the project I'm working in 
setwd(here::here())

# activate the packages as needed
library(tidyverse)
library(INLA)
library(inlabru)
library(fmesher)
# anything spatial relies on sf
library(sf)
```

### Data Checks

I use the same Kilpisjärvi data from previous examples. It has spatial coordinates in EUREF-FIN and WGS84, species richness, individual species' covers (as a %), trait information, and some climate variables. I'll use the WGS84 coordinates along with trait and climate covariates to model species richness.

```{r dataImport, message = FALSE}
# import the data
kjRaw <- read_csv("Kilpisjarvi.csv")
```

There are `r dim(kjRaw)[1]` rows and `r dim(kjRaw)[2]` columns. You should always visualise and explore the data, but I assume that you've done this in the previous exercises up to now. `head()`, `View()`, and `summary()` are a good place to start. There are too many columns for the former two to render well in a notebook like this, but here's an illustrative summary of the latter.

```{r summary}
summary(kjRaw)
```

`INLA` can handle NAs in covariate data but not in the response, so make sure to check for messier data. There is nothing of concern for us if modelling `species_rich` as the response.

```{r naTest, message = FALSE}
# handy shortcut to see NAs per column for the three specific columns of interest
t(kjRaw %>% select(c("Lat", "Lon", "species_rich")) %>% 
    summarise(across(everything(), ~ sum(is.na(.)))))
```

### Spatial Setup

You can skip this if you're fitting a non-spatial model (e.g., GLM or GLMM), but if you're stopping there then why use INLA-SPDE in the first place?

First comes setting up the spatial structure in the data itself because working with `sf` objects makes everything easier.

```{r data, results = FALSE}
# make spatial
kjGeo <- st_as_sf(kjRaw, 
                  coords = c("Lon", "Lat"),
                  crs = 4326)
```

SPDE models start with the mesh, implemented using `fmesher`. At its simplest, you can do this automatically with `sf` objects or raw coordinates. In general, the more mesh nodes there are, the longer the model will take to run. If you want to test lots of models without worrying too much about *how well the estimation works*, coarse meshes are best. 

`fmesher` started as a function within INLA, so older code uses slightly different syntax (more information on converting via the [vignette](https://inlabru-org.github.io/fmesher/articles/inla_conversion.html)). These days `fmesher` can be used to specify spatial fields for other packages like `tinyVAST` and `sdmTMB` too; other packages like `gllvm` and `Hmsc` are working on compatability. `fmesher` builds a mesh based on triangles, where values (e.g., of covariates, posterior estimates, spatial fields) are treated as the same at all points within a single triangle. It's not *truly* continuous in a strict sense, but you can get very close by creating finer and finer meshes.

I'll show a basic, default mesh using the base method. The key things to specify are locations, resolution, and CRS:

* **location** can take either a boundary (`boundary = `) or points (`loc = `). There are ways to do this without an `sf` object, but it's much easier this way. _You can only pick `loc` or `boundary` and **not both**_.
* **resolution**, always in map units, can be set several ways. "Edge" here refers to the triangle edge in a mesh. If you think there are boundary effects, use `max.edge = c(a,b)` to set units both inside and slightly outside your main study area. If not, you can just set `max.edge = a` limited to the whole study area. Its complement `cutoff = a` alternatively sets the minimum edge in the mesh.
* **CRS** is what it sounds like.

Some handy alternatives for more advanced options (all with the `loc =` argument):

* `loc = ` can generate spherical meshes for global models if given 3D coordinates. 2D coordinates are the norm for `sf` objects.
* `fm_nonconvex_hull()` works with non-convex spatial areas that defaults handle poorly.
* `fm_hexagon_lattice()` is Finn's recommendation for a baseline mesh but has been (in his words) ignored by most modellers because it wasn't the default nor in the vignettes until this year. It represents a spatially uniform sampling protocol.

```{r mesh, message = FALSE, fig.width = 10, fig.height = 8, fig.align = "center"}
meshBasic <- fm_mesh_2d_inla(
  # points sf object
  loc = kjGeo$geometry, 
  # set to account for boundary effects
  max.edge = c(0.9, # smaller inner units give a more detailed mesh within the sampling area
               1), # smaller outer units give more detailed spatial estimates of boundary effects
  crs = fm_crs(kjGeo)
)

# visually confirm
meshPlot <- ggplot() + 
  # fmesher plotting function
  geom_fm(data = meshBasic) + 
  # overlay actual points
  geom_sf(data = kjGeo, size = 0.5, colour = "red") + 
  theme_bw()
meshPlot
```

Next, make the SPDE object for spatial models. I again use the base function, but `inla.spde2.pcmatern` is a nice alternative when you're working with big data: you can set **penalised complexity priors** to avoid bloat in the spatial structure. This data is quite compact already, so this isn't necessary here.

You can add additional priors on spatial scale/range ($\kappa$ or $\rho$) and variance ($\tau$ or $\sigma$) depending on whether you're using the [`matern`](https://rdrr.io/github/INBO-BMK/INLA/man/inla.spde2.matern.html) or [`pcmatern`](https://rdrr.io/github/INBO-BMK/INLA/man/inla.spde2.pcmatern.html) functions, respectively. There is also an $\alpha$ parameter directly controlling the SPDE's smoothness (since it's fit using splines) between 0 and 2. $\alpha$ is 2 by default in the 2D case, and you'll rarely need to mess with it for normal spatial models.

As you might tell from the function name, these 2D meshes (and lots of other spatial models in `INLA`) are based on the Matérn covariance function.

```{r spdeSetup, results = FALSE}
# define Matern correlation on the mesh for SPDE
matern2D <- inla.spde2.matern(mesh = meshBasic)
```

And that's it! You can import data and set up a simple model with just 10 lines of code.

### Model Formulation

I'm working with species richness, which is positive discrete (i.e., no values below zero and no decimal values). It varies between `r min(kjRaw$species_rich)` and `r max(kjRaw$species_rich)`.

```{r richPlot, message = FALSE, fig.align = "center", echo = FALSE}
richPlot <- ggplot(kjRaw, aes(x = species_rich)) + geom_histogram() + theme_bw() + 
  xlab("Species richness") + ylab("Count")
richPlot
```

INLA supports an dizzying amount of distribution families that aren't well [documented](https://inla.r-inla-download.org/r-inla.org/doc/likelihood/). In this case, [Gaussians](https://inla.r-inla-download.org/r-inla.org/doc/likelihood/gaussian.pdf) are a decent baseline choice but cannot be constrained to positive values only. [Poisson](https://inla.r-inla-download.org/r-inla.org/doc/likelihood/poisson.pdf) is a common alternative (especially [non-zero Poisson](https://inla.r-inla-download.org/r-inla.org/doc/likelihood/nzpoisson.pdf)) though dispersion is always a concern. [Negative binomial distributions](https://inla.r-inla-download.org/r-inla.org/doc/likelihood/nbinomial.pdf) can present another good option. 

In this case, I use the [Tweedie distribution](https://inla.r-inla-download.org/r-inla.org/doc/likelihood/tweedie.pdf)--which includes Poisson-Gamma, Gaussian, gamma, and quasi-Poisson. Tweedie particularly suits positive continuous and right-skewed distributions that may include zero inflation.[@gilchrist_use_2000] It comes with two hyperparameters: $\theta_1$, power, and $\theta_2$, log(dispersion). Power $p$ is actually $1 + \frac{\exp(\theta_1)}{1+\exp(\theta_1)}$. So when $p = 2$, it's a [quasi-Poisson](https://bookdown.org/mike/data_analysis/sec-quasi-poisson-regression.html) (Poisson with some dispersion); $p = 1$ is a [gamma distribution](https://library.virginia.edu/data/articles/getting-started-with-gamma-regression).

In short, the Tweedie is flexible and relatively easy to interpret. It takes a log-link in INLA, so make sure to exponentiate your coefficients on the response scale. For other data you might use, just note that the Tweedie also has [a scale parameter that determines model convergence](https://groups.google.com/g/r-inla-discussion-group/c/jqsJIa6QWBo). It doesn't matter for results what the term actually is because transformations and back-computations, so play around with it if you're having model convergence issues.

Remember earlier that I mentioned the dangers of collinearity in linear additive regression models used for inference. To decide which covariates to include, I look at the correlation plot. In reality, collinearity is better checked after model fit, but correlation can sometimes (not always) give a good early warning of possible issues. I decide that the community-weighted means (CWM) of leaf dry matter content (LDMC) and leaf area are too closely correlated. Mean soil temperature and frost degree days (FDD) have the same issue. 

```{r covCor, echo = FALSE, message = FALSE, fig.align = "center"}
ggplot(reshape2::melt(cor(kjRaw %>% select_if(is.numeric) %>% select(-c("Lat","Lon","N_TM35FIN","E_TM35FIN",
                                                              "Gersyl_max_height", "Gersyl_LDMC", "Gersyl_leaf_area",
                                                              "Saxifraga_cernua","Rhododendron_lapponicum","Arctous_alpina",
                                                              "Dryas_octopetala","Cornus_suecica","Vaccinium_myrtillus",
                                                              "Empetrum_nigrum","Bistorta_vivipara","Geranium_sylvaticum",
                                                              "Trollius_europaeus")),
                use = "complete.obs")), 
       aes(Var1, Var2, fill=value)) +
  geom_tile(height=0.8, width=0.8) +
  scale_fill_gradient2(low="#fde725", mid="#21918c", high="#440154", midpoint = 0.55) +
  theme_minimal() +
  coord_equal() +
  labs(x="",y="",fill="Correlation") +
  theme(axis.text.x=element_text(size=13, angle=45, vjust=1, hjust=1, 
                                 margin=margin(-3,0,0,0)),
        axis.text.y=element_text(size=13, margin=margin(0,-3,0,0)),
        panel.grid.major=element_blank()) 
```

Finally, modern modelling packages are increasingly sensitive to scaled versus unscaled covariates, and `INLA` is no exception. What to rescale and how is complicated for complex structures (e.g., `sf` and `terra` spatial covariates; temporal structures), but it's straightforward in a normal dataframe like this. However, this affects model interpretation, so I want to retain the original means and variances for later.

```{r norm, message = FALSE}
# rescale manually because scale() is a pain to deal with using tidyverse
kjGeoNorm <- kjGeo %>% mutate(across(c("cwm_max_height", "cwm_LDMC", "FDD_T3", "GDD3_T3", "median_moist"), 
                                     ~ (. - mean(., na.rm = TRUE)) / sd(., na.rm = TRUE)))


# save the mean and standard deviation to convert back for interpretation
mean <- as.data.frame(t(kjRaw %>% select(c("cwm_max_height", "cwm_LDMC", "FDD_T3", "GDD3_T3", "median_moist")) %>% 
    summarise(across(everything(), ~ mean(., na.rm = TRUE))))) %>% rename(mean = "V1")
sd <- as.data.frame(t(kjRaw %>% select(c("cwm_max_height", "cwm_LDMC", "FDD_T3", "GDD3_T3", "median_moist")) %>% 
    summarise(across(everything(), ~ sd(., na.rm = TRUE))))) %>% rename(sd = "V1")

metaStats <- cbind(mean, sd)
metaStats
```

### Model Running

`inlabru` syntax is relatively easy, and I'll exclude the complex options for now. I personally like the `bru_obs()` intermediate function, but it's optional in the newer versions. The `options = list()` argument for `bru()` has too many options to list out, but some that I've used before (and liked) include:

* `lincomb`: maybe you're interested in a linear combination of covariates (e.g., grazing by two different species) which you can add
* `quantiles`: are you unsatisfied with the median or want easier access to credible levels (e.g., 95% CI)?
* `control.compute`: what metrics do you want for model evaluation? CPO, WAIC, DIC, and marginals predictors can all be selected
* `control.predictor`: what do you want for model prediction, if anything?
* `control.inla`: manually choose the fitting strategy (e.g., empirical Bayes to speed things up)
* `bru_verbose`: set to TRUE for basic messages about the fit; set to 2 or 3 for even more details

First, I show a basic non-spatial GLM with `inlabru` syntax. Compare with `bayesreg` or any other regression method of your choice if you'd like.

```{r baseModel, message = FALSE}
# write the formula
formulaGLM <- species_rich ~ Intercept(1) + cwm_max_height + cwm_LDMC + 
  FDD_T3 + GDD3_T3 + median_moist

# run the model
fitGLM <- bru(formulaGLM, kjGeoNorm, family = "tweedie"
              # I really like custom quantiles, so I show the syntax here
              #options = list(quantiles = c(0.025, 0.05, 0.5, 0.95, 0.975)))
              )
```

You can also fit GLMMs with `inlabru` by adding `field(site, model = "iid")` to the formula. Here, I don't bother running it, but you can get out the site names (e.g., "AE09", "MAL") and try a fit if you'd like.

Spatial models aren't that much more complex to build and run. The main difference is the `field` argument, which is the engine of all complex modelling in `inlabru`. Matérn spatial fields with time lags and mixed effects (including hierarchical designs) are among the "simpler" possible designs. There's also no limit on the number of effects, so you can have survey random effects and a spatial random field in the same model. Just make sure to name each effect differently (e.g., `field1()`, `field2()`, `field3()`). As a warning, models with too many fields (or improperly specified ones) may not converge and spit out a warning while failing. The more niche the structure, the more thinking is needed to ensure that things work OK.

```{r spatModel}
# write the formula
formulaGeo <- species_rich ~ Intercept(1) + cwm_max_height + cwm_LDMC + 
  FDD_T3 + GDD3_T3 + median_moist + 
  # new stuff
  field(geometry, model = matern2D)
  # to demonstrate the syntax for adding a second effect
  # + field2(site, model = "iid")

# run the model
fitGeo <- bru(formulaGeo, kjGeoNorm, family = "tweedie")
```

`inlabru` *can* concurrently incorporate more complex effects, but the model specification bloats quickly, and you'll need to use the `bru_obs()` (previously called `like()`) between specifying the formula and fitting the model in order to set a more detailed likelihood. A lot of this functionality is new (especially for `terra` objects), but there are a examples of how to include [spatial covariates in point process models](https://inlabru-org.github.io/inlabruCourseMay2025/articles/lgcp_2d_covars.html). Syntax is broadly the same. I give an example below using the same syntax for spatial field models. Posterior checks are left as an exercise to the reader because of syntax bloat.

It's important to note that any *special* covariates you choose to include, `field` notation is essential. And below is an excellent demonstration that naming doesn't matter. However, when using this syntax, the `model = ` argument tells `inlabru` how to evaluate the object (especially if using a different data source outside the main dataframe). A very poorly organised list of `model` options can be found in the depths of [original `INLA` documentation](https://inla.r-inla-download.org/r-inla.org/doc/latent/). If just trying to fit a fixed effect, `model = "linear"` should work for `terra` objects. If having trouble with spatial objects, play around with manual options in `eval_spatial()` which usually runs in the background.

```{r rastModel, eval = FALSE}
# convert the mesh back to sf
sfMesh <- st_as_sf(as.data.frame(meshBasic$loc), 
                 coords = c("V1", "V2"),
                 crs = 4326)
# get bounding box
meshBounds <- st_as_sfc(st_bbox(sfMesh), crs = 4326)
# extract centroids for elevatr evaluation
meshCentre <- st_centroid(sfMesh)

library(elevatr)
library(terra)
# arbitrary call for demonstrative purposes
elevRL <- get_elev_raster(locations = meshCentre, z = 10)
# convert from RasterLayer to SpatRaster
# elevatr's GitHub says they're changing output object type, so maybe unnecessary in the future
elevT <- crop(terra::rast(elevRL, "epsg:4326"), 
              # crop to bounding box for evaluation
              meshBounds)
# de-mean elevation (copying the example article, but changing the variance may be good too)
elev <- elevT - mean(terra::values(elevT), na.rm = TRUE)

# define the model
formRast <- species_rich ~ elev(elev, # syntax is more complex if your data object has more than 1 layer
                                model = "linear") +
  # add previous model effects
  cwm_max_height + cwm_LDMC + FDD_T3 + GDD3_T3 + median_moist + 
  field(geometry, model = matern2D) + Intercept(1)
# run the model
fitGeoRast <- bru(formRast, kjGeoNorm, family = "tweedie")
```

## Understanding Output

Bayesian models, and especially advanced packages like `inlabru` lack an agreed-upon paradigm for interpretation and evaluation. It's your job as a modeller and scientist to understand what the numbers are saying, why it may matter, and what the pitfalls are. INLA isn't really that different from any other spatial Bayesian model, so you can absolutely steal my code while using your own geographer's knowledge to draw conclusions.

Broadly, `INLA` models have two ways to look at output:

* the information contained directly in the model object like residuals (and other stuff added using `control.compute` and `control.predictor`)
* a newer `predict` function that works only with `inlabru` and not legacy `INLA` models

Both are useful and your focus will depend on the research question (i.e., prediction or inference). The model object itself is quite big, and I won't even pretend to give a deep overview of its contents. These can be used for both interpretation and diagnostics depending on what's of interest.

```{r names}
names(fitGeo)
```

### Interpretation

The following section contains in-line code evaluation. If you're confused about where a number came from, consult the `.rmd` file in [the GitHub repo](https://github.com/MishaTs/DP00BB11_INLA). This section starts on line 341.

Start with the summary function. It gives a good overview.

```{r sum}
summary(fitGeo)
```

In it is most of the relevant non-spatial information. Contrary to what you might think, the only *truly spatial* part of this `inlabru` model is the `field` itself. Everything else functions normally via regression rules. I demonstrate how to interpret the intercept and one coefficient:

* **Intercept**: the typical site species richness when all covariates are at their mean values is `r round(exp(fitGeo$summary.fixed$mean[1]), 2)` with a [`r round(exp(fitGeo$summary.fixed[1,3]), 2)`, `r round(exp(fitGeo$summary.fixed[1,5]), 2)`] 95% credible interval
* **FDD_T3**: when all other covariates are at their mean values, an extra frost degree day leads to `r signif(exp(fitGeo$summary.fixed$mean[2])/sd(kjRaw$FDD_T3, na.rm = TRUE), 2)` `r ifelse(fitGeo$summary.fixed$mean[2] >= 0, "higher", "lower")` species richness with a [`r signif(exp(fitGeo$summary.fixed[2,3])/sd(kjRaw$FDD_T3, na.rm = TRUE), 2)`, `r signif(exp(fitGeo$summary.fixed[2,5])/sd(kjRaw$FDD_T3, na.rm = TRUE), 2)`] 95% credible interval

Those of you with any social sciences background may prefer the more concise Latin term "ceteris paribus" (i.e., "all else held equal") that helps make interpretations like this read more concisely.

The specific formula to convert a coefficient output to "real world" language is `exp(inlaCoefficient)/sd(originalColumn)`. For FDD_T3, this is `exp(fitGeo$summary.fixed$mean[2])/sd(kjRaw$FDD_T3, na.rm = TRUE)`. Rinse and repeat with the credible interval bounds that you're interested in since they convert the same as coefficients. If you want to use the original values, you can say that a 1 standard deviation increase in FDDs leads to an `exp(inlaCoefficient)`, in this case `r signif(exp(fitGeo$summary.fixed$mean[2]), 2)`, `r ifelse(fitGeo$summary.fixed$mean[2] >= 0, "higher", "lower")` species richness. If you prefer social science language, then a 1 standard deviation increase in FDDs leads to a `r abs(signif(fitGeo$summary.fixed$mean[2], 2))`% `r ifelse(fitGeo$summary.fixed$mean[2] >= 0, "increase", "decrease")` in species richness (just the actual model coefficient itself). Communicate in the way that feels best to you.

Remember, I'm exponentiating because the Tweedie distribution takes a log link function in `INLA`. You can skip this step if using a distribution with a linear link, like Gaussian. If you're working with logistic link functions, I can only wish you good luck. Things also change a bit if you rescale or log your response variable (which can be done), but the general interpretative rules stay the same: consult these handy StackExchange posts for [quantiative](https://stats.stackexchange.com/questions/603213/interpretation-of-rescaled-coefficients-in-ols) and [language](https://stats.stackexchange.com/questions/407822/interpretation-of-standardized-z-score-rescaled-linear-model-coefficients) tips on interpreting models where both response variables and covariates are rescaled.

There is no "significance" in Bayesian statistics in the same way as classical methods, but it's quite common to say that a 95% credible interval excluding zero shows meaningful evidence of an effect. This is why I suggested (and provided code for) different quantiles in the `bru` call: you can also do this at the 90% credible level, for example.

Dealing with the spatial field is a bit more complicated and best handled using the `predict` function. This is relatively new: example articles and vignettes are slowly showing the ways it can be used to replace traditional model outputs for more robust estimation. Predictions are especially useful at understanding the model's spatial behaviour. I show the mean model prediction when all covariates are at their average values (including the spatially variant field and the fixed intercept).

```{r bounds, echo = FALSE}
# run this time because I didn't evaluate it previously
sfMesh <- st_as_sf(as.data.frame(meshBasic$loc), 
                 coords = c("V1", "V2"),
                 crs = 4326)

meshBounds <- st_as_sfc(st_bbox(sfMesh), crs = 4326)
```

```{r spatPlot, fig.align = "center", warning = FALSE}
# create mesh only within the study area
ppxl <- fm_pixels(meshBasic, mask = meshBounds)

# run the built-in inlabru function to get predictions
predField <- predict(
  fitGeo, ppxl,
  # look at the field only; add more covariates using typical "+" notation for full model predictions
  ~ field + Intercept
)

# modify data to get it to the response scale
predFieldResponse <- predField %>% mutate(linkMean = exp(mean),
                                          # not used, but you can also plot CIs
                                          linkLower = exp(q0.025),
                                          linkUpper = exp(q0.975))

# plot
spatPlot <- ggplot() +
  # plotting as tile looks best
  gg(predFieldResponse %>% relocate(linkMean), geom = "tile") +
  scale_fill_viridis_c(begin = 0, end = 1) + 
  theme_bw() + 
  theme(axis.title = element_blank(),
        panel.grid = element_line(colour = "#ABB0B8",
                                  linetype = 2),
        # put gridlines on top for better map
        panel.ontop = TRUE,
        panel.background = element_rect(color = NA, fill = NA)) + 
  labs(fill = "Base spatial richness")

spatPlot
```

You can also look at the underlying parameters driving spatial field behaviour, mainly the range and variance. These can be constrained with priors using the two `inla.spde2` functions if they look unsatisfactory (e.g., if you want a more constrained variance or broader range). The distributions' shapes look the same because of the underlying SPDE model architecture.

```{r rangePlot, fig.align = "center"}
# use the spde.posterior to extract the full distribution of parameters of interest
spde.range <- spde.posterior(fitGeo, "field", what = "range")
# this is a specialised plot function that takes ggplot syntax
range.plot <- plot(spde.range) + theme_bw()
range.plot
```

```{r varPlot, fig.align = "center"}
spde.logvar <- spde.posterior(fitGeo, "field", what = "variance")
var.plot <- plot(spde.logvar) + theme_bw()
var.plot
```

Bayes factor is another measure for inference which is the ratio of the marginal likelihoods (`mlik`) of two models. From the model object, you want the ["log marginal-likelihood (integration)"](https://groups.google.com/g/r-inla-discussion-group/c/1Zs00DttjEE). So if you want to measure the relative importance of a single term, you must fit models with and without the term of interest. [Wikipedia explains](https://en.wikipedia.org/wiki/Bayes_factor#Interpretation) how to interpret this well with good scientific references, but there are no commonly agreed-upon theoretical values (and be careful of which logarithmic base the table is in). But for example, you can compare models with and without spatial fields to see if the more complex structure is advisable. Its use remains debated, and some ponder if a likelihood comparison (i.e., not considering the priors and their contribution to a Bayesian posterior) is really the right way to go.[@gelman_bayesian_2015] 

To demonstrate, I consider models with or without a spatial field using the log likelihoods: `r round(fitGeo$mlik[1], 2)` for the spatial model and `r round(fitGLM$mlik[1], 2)` for the basic one. Their ratio is `r round(fitGeo$mlik[1]/fitGLM$mlik[1], 2)`, which weakly/moderately supports the inclusion of a spatial term. My model is simple, so this isn't too shocking. 

### Diagnostics

"Since all models are wrong the scientist must be alert to what is importantly wrong." - George Box[@box_science_1976]

Stationarity (weak) and isotropy (strong) assumptions are best tackled theoretically by most users. Is the model missing any linear fixed effects or spatial variance terms that would affect the spatial patterns across the pixels/points (i.e., stationarity) or across the whole study area in larger space (i.e. isotropy)? For example, maybe georichness is a strong spatial explainer of species richness in the Kilpisjärvi area that's not included in basic topography. Ordination methods (e.g., `gllvm`) on residuals can offer a nice way to examine the unmodelled structures.

Predictive scoring, if that's your focus, is a complex topic, and `inlabru` has a nice theoretical description and links to another package, `scoringRules` that can provide more help if you consult [the official article](https://inlabru-org.github.io/inlabru/articles/prediction_scores.html). Another older resource is [the INLA book](https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#sec:modelassess). These generally require some testing set (data that you do not use to fit the model but generated in the same way) or at least a theoretically generated comparison dataset. Both of these are too much work for this exercise. In my experience, CPRS is often treated as a "gold standard" for predictive metrics. Sometimes, these are used as model evaluation for inference as well due to their good theoretical and practical implementation.

Easier options to look at are CPO, WAIC, and DIC which are not computed by default and must be selected using `control.compute` (I did not do this). Residuals, too, must be enabled with `control.compute`; deviance residuals are the default and easy to get. In classical statistics, deviance residuals are a more general form of Pearson residuals and better account for distributions with mean-variance relationships.

If you're working with point process models (i.e., modelling the specific spatial realisation of a process), then all of the default `INLA` options are not advised. [Information criteria](https://groups.google.com/g/r-inla-discussion-group/c/EppMtvdJ610/m/YwjpRvoAAgAJ) and [CPO](https://groups.google.com/g/r-inla-discussion-group/c/H99TgXGhI6M/m/cY5Zo9nCAgAJ) both show some issues, and a more targeted [residual analysis tutorial](https://inlabru-org.github.io/inlabru/articles/2d_lgcp_residuals_sf.html) can guide your evaluation.

```{r rerun}
# rerun the model to get everything for posterior checks
fitGeoDiag <- bru_rerun(fitGeo,
                        options = list(control.compute = list(waic = TRUE, 
                                                              cpo = TRUE, 
                                                              dic = TRUE, 
                                                              return.marginals.predictor = TRUE,
                                                              residuals = TRUE)))
```

For simple conclusions, scoring criteria analysis and residual examination works exactly the same as elsewhere. All of these correspond perfectly to the number of rows in the base data (i.e., 1 score or residual per observation), so it's easy to add it back to the original data and visualise as you see fit. 

Plotting against observed richness, it looks like the spatial model should be refined. The gap around 0 isn't an issue and [an effect of the log transformation](https://groups.google.com/g/r-inla-discussion-group/c/572O2Nrev5Y/m/QaxKIiibFQAJ). But the model both underestimates higher richness and overestimates lower richness. This isn't that shocking: a simple model will pull everything towards the mean value. The data has no 0 values; if it did, the strong negative residuals for low richness might motivate a zero-inflated distribution. 

```{r residInspect, fig.align = "center"}
kjRes <- cbind(kjGeo, fitGeoDiag$residuals$deviance.residuals)

ggplot(kjRes, aes(y = fitGeoDiag.residuals.deviance.residuals, x = species_rich)) +
  geom_point() + 
  theme_bw() + 
  ylab("Deviance residuals") + xlab("Species richness")
```

If the model over and underpredicts richness at roughly equal rates, then that's generally ok. But if residuals are imbalanced, then that's a diagnostic issue indicating possible bias. In this case the model slightly overpredicts low values more often and more extremely than its underpredictions for large values. Thankfully the degree to which this happens isn't that extreme: personally, I think a plot like this is fine.

```{r residHist, fig.align = "center"}
ggplot(kjRes, aes(x = fitGeoDiag.residuals.deviance.residuals)) +
  geom_histogram(binwidth = 0.4) + 
  theme_bw() + 
  xlab("Spatial model deviance residuals")
```

Getting fitted values is a bit more complicated because the `summary.fitted.values` do not match 1:1 the raw data. In this case there are `r nrow(fitGeoDiag$summary.fitted.values)` fitted values compared against `r nrow(kjRaw)` raw observations. The best way to do get fitted values is to feed in the original data to the `predict` function to ensure they can be combined together. You can then compute (prediction) residuals using your favourite method of choice. Try out a residuals versus fitted plot if you'd like to explore a more traditional plot for model diagnostics.

Finally, it's always good practice to look at the distribution posterior hyperparameters: was Tweedie a good choice? First, the dispersion parameter varies little around its mean, `r signif(fitGeo$summary.hyperpar$mean[2], 2)`. So Poisson would have performed poorly. The power parameter behaves similarly around its mean, `r signif(fitGeo$summary.hyperpar$mean[1], 2)`: the Tweedie ends up neither a full gamma or quasi-Poisson distribution, and simplifying it to one of its constituent distributions is probably a poor idea. So the model does a decent job specifying an effective mean-variance relationship by picking a suitable distribution. These numbers are all found in the first `summary()` call but can be directly accessed using `fitGeo$summary.hyperpar`. 

These sorts of simpler 2D spatial field models can be interpreted the same as any other regression along with some notable hyperparameter checks. However, you can and should go as deep as you want. Sensitivity analysis is a relatively low-cost way of seeing how changing (hyper)parameters, formulations, and priors will affect results and lead to a more sensible model. If your dataset is small enough to allow lots of model exploration, definitely give it a try.

## Wrap-Up

`INLA` is a powerful method where you can fit a dizzying variety of models. I don't cover it here, but it can even accommodate spatio-temporal data, either through mixed effects or autoregressive structures in the spatial field. More complex spatial relationships are also possible like [spatially varying covariates (SVC)](https://inlabru-org.github.io/inlabru/articles/svc.html). For more information on the options, explore [the package website](https://inlabru-org.github.io/inlabru/index.html). 

Now is a time of change for `inlabru` as the package is going through major updates and developments to make it easier to use and understand. Last time I ran these models a year ago, the syntax was almost 5 times as long. Before `inlabru` came out, the original `INLA` code took over 100 lines for a single model, including defining matrices manually. If you plan to consult these notes 3-4 years from now, it's worth checking the website first.

Feel free to [email me](mailto:Michael.Tseitlin@oulu.fi) or ask today/tomorrow if you think some of these apply to your own research and you want more help.

If you'd rather get help from the source, `INLA` has [a great Q&A discussion forum](https://groups.google.com/g/r-inla-discussion-group) where Finn and Håvard (the original authors and package developers) regularly comment and troubleshoot code from the community. When in doubt, just ask! It's also possible you're not the first one to have a problem, so maybe you'll just find an answer by searching through it.

## Bonus: Model Equations

Each `INLA` model has its own structure depending on the distribution family and structure. The part two model from Villejo et al. (2023) is a good guide to basic spatial structure modelling.[@villejo_data_2023] An original tutorial paper through base `INLA` with model equations is syntactically dated but can serve as another good guide for the underlying formulae.[@blangiardo_spatial_2015] Both of these references specifically describe spatiotemporal models.

This example only has a spatial effect, so the specification is slightly simpler:

$$log(\text{Richness}_i) = \beta_0 + \beta_1*\text{CWM Max Height}_i + \beta_2*\text{CWM LDMC}_i + \beta_3*\text{FDD}_i + \beta4*\text{GDD} + \beta_5 * \text{Median Moisture}_i + \xi_i$$
where the $\beta$ coefficients represent the effect of a fixed covariate for our terms of interest and $\xi$ is a spatially structured term for the latent Gaussian estimated via SPDE: Villejo et al. (2023) would call this a structured spatial effect $\psi_i$. Note that the core responses $\text{Richness}_i$ are assumed to be conditionally independent given the effects: they ought not be structurally dependent in a way that you exclude from the model.

INLA's definition of Tweedie is a bit messy for most casual users:

$$Tw(\alpha, \gamma, \lambda) = \sum_{i=1}^{N} X_i, \quad \{X_i\} \overset{\mathrm{iid}}{\sim} \Gamma(\alpha,\gamma), \quad N \overset{\perp}{\sim} Poisson(\lambda)$$

using iid gamma and independent Poisson variables to define model components.[@inla_development_team_tweedie_nodate] In `INLA`, $Y$ has mean $\mu = \lambda \alpha \gamma$, power parameter $p = \frac{\alpha+2}{\alpha+1}$, and dispersion $\frac{\phi}{w} = \frac{\lambda^{1-p}(\alpha \gamma)^{2-p}}{2-p}$ including a fixed positive term $w$, a dispersion-specific scaling parameter.[@inla_development_team_tweedie_nodate] This forces $Y$'s mean $\mu$ to be positive and gives variance $\frac{\phi}{w}\mu^p$.[@inla_development_team_tweedie_nodate] 

The richnesses are drawn from this distribution, so $\text{Richness}_i \sim Tw(\alpha, \gamma, \lambda)$.

To give even more implementation detail, Tweedie specifies model hyperparameters $(\theta_1,\theta_2)$ as

$$p = 1 + \frac{\exp(\theta_1)}{1+\exp(\theta_1)}, \quad 1 < p < 2$$

and

$$\phi = \exp(\theta_2), \quad \phi > 0$$

where $p = 1$ simplifies to a (quasi)Poission distribution depending on dispersion, and $p = 2$ simplifies to a Gamma distribution.[@inla_development_team_tweedie_nodate]

The Tweedie a user encounters in `INLA` is practically specified using $Tw(\theta_1, \theta_2, w)$ but still using the underlying distributional structure above. As the dispersion parameter is de-linked from the power parameter when specifying hyperparameters, you should take care not to violate any underlying distributional structures in manual hyperprior specifications.

## Acknowledgements

Many of the links, references, and materials came from [Ben Swallow](https://ben-swallow-research.github.io/) while supervising my MSc dissertation. Newer `inlabru` materials came from Finn Lindgren's [2025 `inlabru` course](https://inlabru-org.github.io/inlabruCourseMay2025/index.html).

# References
